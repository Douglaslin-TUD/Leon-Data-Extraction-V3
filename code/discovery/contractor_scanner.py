#!/usr/bin/env python3
"""
Contractor Data Scanner - Phase 1: Discovery
æ‰«æcontractoråŸå§‹Excelæ–‡ä»¶ï¼Œæå–å­—æ®µç»“æ„å’Œå”¯ä¸€å€¼

åªå…³æ³¨15ä¸ªå…³é”®å­—æ®µçš„æ˜ å°„
"""

import pandas as pd
import openpyxl
from pathlib import Path
import json
from datetime import datetime
from collections import defaultdict
import re

# 15ä¸ªå…³é”®å­—æ®µï¼ˆä»field_mapping_2022.jsonæå–ï¼‰
CRITICAL_FIELDS = {
    2: {"nl": "DISTRICT", "en": "District", "cn": "åŒºåŸŸ"},
    3: {"nl": "ZAAKNUMMER", "en": "Case Number", "cn": "æ¡ˆä»¶ç¼–å·"},
    4: {"nl": "Weg", "en": "Road Number", "cn": "é“è·¯ç¼–å·"},
    5: {"nl": "BAAN", "en": "Carriageway", "cn": "è½¦é“ç±»å‹"},
    6: {"nl": "WEGLET", "en": "Road Letter", "cn": "é“è·¯å­—æ¯æ ‡è¯†"},
    7: {"nl": "VAN", "en": "From Position", "cn": "èµ·å§‹ä½ç½®"},
    8: {"nl": "TOT", "en": "To Position", "cn": "ç»“æŸä½ç½®"},
    9: {"nl": "STROOK", "en": "Lane Number", "cn": "è½¦é“ç¼–å·"},
    10: {"nl": "Aantal rijstroken", "en": "Number of Lanes", "cn": "è½¦é“æ•°é‡"},
    11: {"nl": "KM_Van", "en": "Kilometer From", "cn": "èµ·ç‚¹å…¬é‡Œæ•°"},
    12: {"nl": "KM_Tot", "en": "Kilometer To", "cn": "ç»ˆç‚¹å…¬é‡Œæ•°"},
    13: {"nl": "Lengte", "en": "Length", "cn": "é•¿åº¦"},
    15: {"nl": "MENGSELCODE", "en": "Mixture Code", "cn": "æ··åˆæ–™ä»£ç "},
    17: {"nl": "DEKLAAGSOORT", "en": "Surface Layer Type", "cn": "é¢å±‚ç±»å‹"},
    22: {"nl": "AANLEGDATUM", "en": "Construction Date", "cn": "æ–½å·¥æ—¥æœŸ"}
}


class ContractorScanner:
    """æ‰«æcontractoræ–‡ä»¶å¹¶æå–æ•°æ®"""

    def __init__(self, base_path):
        self.base_path = Path(base_path)
        self.results = {
            "scan_date": datetime.now().isoformat(),
            "total_files_scanned": 0,
            "contractors": {}
        }

    def find_contractor_files(self):
        """æŸ¥æ‰¾æ‰€æœ‰contractoråŸå§‹æ–‡ä»¶"""
        files_2021 = list(self.base_path.glob("2021/Origineel/*.xlsx"))
        files_2022 = list(self.base_path.glob("2022/Origineel/*.xlsx"))

        # è¿‡æ»¤æ‰ä¸´æ—¶æ–‡ä»¶å’Œéšè—æ–‡ä»¶
        all_files = [f for f in files_2021 + files_2022
                     if not f.name.startswith('~') and not f.name.startswith('.')]

        print(f"æ‰¾åˆ° {len(files_2021)} ä¸ª2021å¹´æ–‡ä»¶")
        print(f"æ‰¾åˆ° {len(files_2022)} ä¸ª2022å¹´æ–‡ä»¶")
        print(f"æ€»è®¡: {len(all_files)} ä¸ªcontractoræ–‡ä»¶")

        return sorted(all_files)

    def identify_data_structure(self, file_path):
        """
        è¯†åˆ«Excelæ–‡ä»¶çš„æ•°æ®ç»“æ„
        è¿”å›: (header_row_index, data_start_row, sheet_name)
        """
        try:
            # è¯»å–å‰20è¡Œï¼Œå¯»æ‰¾è¡¨å¤´
            df_preview = pd.read_excel(file_path, nrows=20, header=None)

            # å¯»æ‰¾è¡¨å¤´è¡Œï¼ˆåŒ…å«å¤šä¸ªéç©ºå•å…ƒæ ¼çš„è¡Œï¼‰
            for idx, row in df_preview.iterrows():
                non_null_count = row.notna().sum()
                if non_null_count >= 10:  # å‡è®¾è¡¨å¤´è‡³å°‘æœ‰10ä¸ªå­—æ®µ
                    header_row = idx
                    data_start_row = idx + 1

                    # è·å–sheetåç§°
                    xl_file = pd.ExcelFile(file_path)
                    sheet_name = xl_file.sheet_names[0]

                    return header_row, data_start_row, sheet_name

            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œé»˜è®¤ç¬¬ä¸€è¡Œæ˜¯è¡¨å¤´
            return 0, 1, pd.ExcelFile(file_path).sheet_names[0]

        except Exception as e:
            print(f"  âš ï¸  è¯†åˆ«æ•°æ®ç»“æ„å¤±è´¥: {e}")
            return 0, 1, "Sheet1"

    def extract_field_names(self, file_path, header_row):
        """æå–å­—æ®µå"""
        try:
            df = pd.read_excel(file_path, header=header_row, nrows=0)
            return list(df.columns)
        except Exception as e:
            print(f"  âš ï¸  æå–å­—æ®µåå¤±è´¥: {e}")
            return []

    def fuzzy_match_field(self, original_field_name, threshold=70):
        """
        æ¨¡ç³ŠåŒ¹é…å­—æ®µååˆ°15ä¸ªå…³é”®å­—æ®µ
        è¿”å›: (field_number, confidence) æˆ– (None, 0)
        """
        original_lower = str(original_field_name).lower().strip()

        best_match = None
        best_score = 0

        for field_num, field_info in CRITICAL_FIELDS.items():
            # æ£€æŸ¥è·å…°è¯­åç§°
            nl_name = field_info["nl"].lower()

            # ç²¾ç¡®åŒ¹é…
            if original_lower == nl_name:
                return field_num, 100

            # åŒ…å«åŒ¹é…
            if nl_name in original_lower or original_lower in nl_name:
                score = 85
                if score > best_score:
                    best_score = score
                    best_match = field_num

            # å…³é”®è¯åŒ¹é…
            keywords = {
                2: ["district"],
                3: ["zaak", "nummer"],
                4: ["weg"],
                5: ["baan", "carriageway"],
                6: ["weglet", "letter"],
                7: ["van", "from", "start"],
                8: ["tot", "to", "end"],
                9: ["strook", "lane"],
                10: ["aantal", "rijstroken", "lanes"],
                11: ["km", "van", "from"],
                12: ["km", "tot", "to"],
                13: ["lengte", "length"],
                15: ["mengsel", "code", "mixture"],
                17: ["deklaag", "surface", "zoab", "sma"],
                22: ["datum", "date", "aanleg", "construction"]
            }

            if field_num in keywords:
                for keyword in keywords[field_num]:
                    if keyword in original_lower:
                        score = 75
                        if score > best_score:
                            best_score = score
                            best_match = field_num

        if best_score >= threshold:
            return best_match, best_score
        else:
            return None, 0

    def extract_unique_values(self, df, field_name, max_samples=100):
        """
        æå–å­—æ®µçš„å”¯ä¸€å€¼å’Œé¢‘ç‡
        åªä¿ç•™å‰max_samplesä¸ªæœ€å¸¸è§çš„å€¼
        """
        try:
            if field_name not in df.columns:
                return {
                    "unique_count": 0,
                    "null_count": 0,
                    "sample_values": [],
                    "top_values": {}
                }

            series = df[field_name]

            # ç»Ÿè®¡
            unique_values = series.dropna().unique()
            value_counts = series.value_counts()

            # å–å‰max_samplesä¸ªæœ€å¸¸è§çš„å€¼
            top_values = value_counts.head(max_samples).to_dict()

            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            top_values_serializable = {
                str(k): int(v) for k, v in top_values.items()
            }

            sample_values = [str(v) for v in unique_values[:20]]  # å‰20ä¸ªæ ·æœ¬

            return {
                "unique_count": len(unique_values),
                "null_count": int(series.isna().sum()),
                "null_percentage": float(series.isna().sum() / len(series) * 100),
                "sample_values": sample_values,
                "top_values": top_values_serializable
            }

        except Exception as e:
            print(f"  âš ï¸  æå–å”¯ä¸€å€¼å¤±è´¥: {field_name}, {e}")
            return {
                "unique_count": 0,
                "null_count": 0,
                "sample_values": [],
                "top_values": {}
            }

    def scan_contractor_file(self, file_path):
        """æ‰«æå•ä¸ªcontractoræ–‡ä»¶"""
        contractor_name = file_path.stem
        year = "2021" if "2021" in str(file_path) else "2022"

        print(f"\nğŸ“‚ æ‰«æ: {contractor_name} ({year})")
        print(f"   æ–‡ä»¶: {file_path.name}")

        try:
            # 1. è¯†åˆ«æ•°æ®ç»“æ„
            header_row, data_start_row, sheet_name = self.identify_data_structure(file_path)
            print(f"   è¡¨å¤´è¡Œ: {header_row + 1}, æ•°æ®èµ·å§‹è¡Œ: {data_start_row + 1}, Sheet: {sheet_name}")

            # 2. è¯»å–å®Œæ•´æ•°æ®
            df = pd.read_excel(file_path, header=header_row)
            total_rows = len(df)
            print(f"   æ€»è¡Œæ•°: {total_rows}")

            # 3. æå–å­—æ®µå
            original_fields = list(df.columns)
            print(f"   åŸå§‹å­—æ®µæ•°: {len(original_fields)}")

            # 4. åŒ¹é…å…³é”®å­—æ®µ
            discovered_critical_fields = []
            matched_count = 0

            for col_idx, original_field in enumerate(original_fields):
                # å°è¯•åŒ¹é…åˆ°15ä¸ªå…³é”®å­—æ®µ
                field_num, confidence = self.fuzzy_match_field(original_field)

                if field_num is not None:
                    matched_count += 1

                    # æå–è¯¥å­—æ®µçš„å”¯ä¸€å€¼
                    value_stats = self.extract_unique_values(df, original_field)

                    field_info = {
                        "original_field_name": str(original_field),
                        "column_index": col_idx,
                        "standard_field_number": field_num,
                        "standard_field_name_nl": CRITICAL_FIELDS[field_num]["nl"],
                        "standard_field_name_cn": CRITICAL_FIELDS[field_num]["cn"],
                        "mapping_confidence": confidence,
                        "mapping_method": "exact_match" if confidence == 100 else "fuzzy_match",
                        "data_type": str(df[original_field].dtype),
                        "unique_count": value_stats["unique_count"],
                        "null_count": value_stats["null_count"],
                        "null_percentage": round(value_stats["null_percentage"], 2),
                        "sample_values": value_stats["sample_values"],
                        "top_values": value_stats["top_values"]
                    }

                    discovered_critical_fields.append(field_info)

            print(f"   âœ… åŒ¹é…åˆ° {matched_count}/15 ä¸ªå…³é”®å­—æ®µ")

            # 5. ä¿å­˜ç»“æœ
            contractor_id = f"{contractor_name}_{year}"
            self.results["contractors"][contractor_id] = {
                "contractor_name": contractor_name,
                "year": year,
                "file_path": str(file_path.relative_to(self.base_path.parent.parent)),
                "sheet_name": sheet_name,
                "header_row": header_row,
                "data_start_row": data_start_row,
                "total_rows": total_rows,
                "original_field_count": len(original_fields),
                "matched_critical_fields": matched_count,
                "discovered_fields": discovered_critical_fields
            }

            self.results["total_files_scanned"] += 1

            return True

        except Exception as e:
            print(f"   âŒ æ‰«æå¤±è´¥: {e}")
            return False

    def generate_value_aggregation(self):
        """
        èšåˆæ‰€æœ‰contractorçš„å­—æ®µå€¼ï¼ŒæŒ‰æ ‡å‡†å­—æ®µåˆ†ç»„
        ç”Ÿæˆ contractor_value_discovery.json
        """
        value_aggregation = {}

        # æŒ‰æ ‡å‡†å­—æ®µåˆ†ç»„
        for field_num, field_info in CRITICAL_FIELDS.items():
            field_name = field_info["nl"]
            field_cn = field_info["cn"]

            value_aggregation[field_name] = {
                "standard_field_number": field_num,
                "standard_field_name_nl": field_name,
                "standard_field_name_cn": field_cn,
                "contractors": {},
                "all_unique_values": set(),
                "total_occurrences": 0
            }

        # æ”¶é›†æ•°æ®
        for contractor_id, contractor_data in self.results["contractors"].items():
            for field in contractor_data["discovered_fields"]:
                std_field_num = field["standard_field_number"]
                std_field_name = CRITICAL_FIELDS[std_field_num]["nl"]

                # æ·»åŠ contractoræ•°æ®
                value_aggregation[std_field_name]["contractors"][contractor_id] = {
                    "original_field_name": field["original_field_name"],
                    "unique_count": field["unique_count"],
                    "null_percentage": field["null_percentage"],
                    "top_values": field["top_values"],
                    "sample_values": field["sample_values"]
                }

                # æ”¶é›†æ‰€æœ‰å”¯ä¸€å€¼
                for val in field["sample_values"]:
                    value_aggregation[std_field_name]["all_unique_values"].add(val)

                # ç»Ÿè®¡æ€»å‡ºç°æ¬¡æ•°
                for count in field["top_values"].values():
                    value_aggregation[std_field_name]["total_occurrences"] += count

        # è½¬æ¢setä¸ºlistï¼ˆJSONå¯åºåˆ—åŒ–ï¼‰
        for field_name in value_aggregation:
            value_aggregation[field_name]["all_unique_values"] = sorted(
                list(value_aggregation[field_name]["all_unique_values"])
            )
            value_aggregation[field_name]["unique_value_count"] = len(
                value_aggregation[field_name]["all_unique_values"]
            )

        return value_aggregation

    def save_results(self, output_dir):
        """ä¿å­˜æ‰«æç»“æœ"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # 1. ä¿å­˜å­—æ®µå‘ç°ç»“æœ
        discovery_file = output_path / "contractor_field_discovery.json"
        with open(discovery_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… å­—æ®µå‘ç°ç»“æœå·²ä¿å­˜: {discovery_file}")

        # 2. ç”Ÿæˆå¹¶ä¿å­˜å€¼èšåˆç»“æœ
        value_aggregation = self.generate_value_aggregation()
        value_file = output_path / "contractor_value_discovery.json"
        with open(value_file, 'w', encoding='utf-8') as f:
            json.dump(value_aggregation, f, ensure_ascii=False, indent=2)
        print(f"âœ… å€¼èšåˆç»“æœå·²ä¿å­˜: {value_file}")

        # 3. ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
        print(f"\nğŸ“Š æ‰«ææ±‡æ€»:")
        print(f"   æ€»æ–‡ä»¶æ•°: {self.results['total_files_scanned']}")
        print(f"   æ€»contractoræ•°: {len(self.results['contractors'])}")

        # ç»Ÿè®¡æ¯ä¸ªå…³é”®å­—æ®µçš„åŒ¹é…ç‡
        field_match_stats = defaultdict(int)
        for contractor_data in self.results["contractors"].values():
            for field in contractor_data["discovered_fields"]:
                field_num = field["standard_field_number"]
                field_match_stats[field_num] += 1

        print(f"\n   å…³é”®å­—æ®µåŒ¹é…ç»Ÿè®¡:")
        for field_num in sorted(CRITICAL_FIELDS.keys()):
            field_name = CRITICAL_FIELDS[field_num]["cn"]
            match_count = field_match_stats.get(field_num, 0)
            total = self.results['total_files_scanned']
            percentage = (match_count / total * 100) if total > 0 else 0
            print(f"   {field_num:2d}. {field_name:15s}: {match_count:2d}/{total:2d} ({percentage:5.1f}%)")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("Contractoræ•°æ®æ‰«æå™¨ - Phase 1: Discovery")
    print("åªæ‰«æ15ä¸ªå…³é”®å­—æ®µ")
    print("=" * 80)

    # è®¾ç½®è·¯å¾„
    base_path = Path("/data/AI Life/Work/RWS_Road_Engineer/01-Projects/Leon_Data_Extraction/Data/FW_ /Validator")
    output_dir = Path("/data/AI Life/Work/RWS_Road_Engineer/01-Projects/Leon_Data_Extraction/Analysis/Contractor_Discovery")

    # åˆ›å»ºæ‰«æå™¨
    scanner = ContractorScanner(base_path)

    # æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶
    contractor_files = scanner.find_contractor_files()

    if not contractor_files:
        print("âŒ æœªæ‰¾åˆ°contractoræ–‡ä»¶ï¼")
        return

    # æ‰«ææ‰€æœ‰æ–‡ä»¶
    print(f"\nå¼€å§‹æ‰«æ {len(contractor_files)} ä¸ªcontractoræ–‡ä»¶...")
    print("=" * 80)

    success_count = 0
    for file_path in contractor_files:
        if scanner.scan_contractor_file(file_path):
            success_count += 1

    print("\n" + "=" * 80)
    print(f"æ‰«æå®Œæˆ: {success_count}/{len(contractor_files)} ä¸ªæ–‡ä»¶æˆåŠŸ")
    print("=" * 80)

    # ä¿å­˜ç»“æœ
    scanner.save_results(output_dir)

    print("\nâœ… Phase 1 å®Œæˆï¼")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   - contractor_field_discovery.json (å­—æ®µå‘ç°)")
    print(f"   - contractor_value_discovery.json (å€¼èšåˆ)")


if __name__ == "__main__":
    main()
